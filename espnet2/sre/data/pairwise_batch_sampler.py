from pathlib import Path
from typing import Union

import numpy as np
import torch
import logging

from espnet2.samplers.abs_sampler import AbsSampler
from espnet2.sre.utils import read_utt2spk


class PairwiseBatchSampler(AbsSampler):
    """BatchSampler which generates tuples of samples from the same class

    Samples generated by this class are tuples that contain samples from the same class (i.e. positive pair).
    In addition, there are no overlaps of class among tuples contained in the same batch.
    (i.e. any pair of tuples from the same batch is negative pair)
    This class is convenient for distance learning such as Siamese, Triplet, and prototype losses.

    Args:
        batch_size:
        key_file:
        utt2spk:
        shuffle:
        distributed:
        num_pair:           number of samples in each tuples.
                            e.x. 1 for softmax, 2 for siamese, >=3 for triplet or proto-typical losses
        max_seg_per_spk:    maximum number of sampler per each class
    """
    
    def __init__(
        self,
        batch_size: int,
        key_file: Union[str, Path],
        utt2spk: Union[str, Path],
        num_pair: int,
        max_seg_per_spk: int,
        shuffle: bool = False,
        distributed: bool = False,
        prevent_same_speaker = True,
    ):
        self.num_pair = num_pair
        self.distributed = distributed
        self.shuffle = shuffle
        self.batch_size = batch_size
        self.max_seg_per_spk = max_seg_per_spk

        # NOTE(kamo): The utts in utt2spk can be superset of key_file.
        #   e.g. The union of training set and validation set
        self.utt2spk_dict, spk2utt_dict, _, _, = read_utt2spk(utt2spk)
        self.num_spk = len(spk2utt_dict)

        self.spk2utt_dict_closed = {}
        with open(key_file, encoding="utf-8") as f:
            self.utt_list = []
            for line in f:
                utt, *spk = line.split(maxsplit=1)
                if utt not in self.utt2spk_dict:
                    raise RuntimeError(f"Unknown key: {utt}")
                self.utt_list.append(utt)
                spk = self.utt2spk_dict[utt]
                self.spk2utt_dict_closed.setdefault(spk, []).append(utt)
        if len(self.utt_list) == 0:
            raise RuntimeError(f"Empty file: {key_file}")
        if len(self.spk2utt_dict_closed) < self.num_pair:
            raise RuntimeError(
                f"Too few speakers: {len(self.spk2utt_dict_closed)} <= {self.num_pair}"
            )
        # NOTE(kamo): Always drop last if not divisible if num_batches != 1
        #   (At least 1 number of batches here)
        self.num_batches = max(len(self.utt_list) // self.batch_size, 1)
        if self.num_batches == 1:
            self.batch_size = len(self.utt_list)

        if self.distributed:
            ws = torch.distributed.get_world_size()
            if ws > batch_size:
                raise RuntimeError(
                    "World size is larger than batch_size: " f"{ws} > {batch_size}"
                )
            if ws > len(self.utt_list):
                raise RuntimeError(
                    "World size is larger than the number of utterances: "
                    f"{ws} > {len(self.utt_list)}"
                )
        self.prevent_same_speaker = prevent_same_speaker

    def __repr__(self):
        _ret = f"{self.__class__.__name__}("
        _ret += f"num_spk={self.num_spk}, "
        _ret += f"num_spk_closed={len(self.spk2utt_dict_closed)}, "
        _ret += f"num_utts={len(self.utt_list)}, "
        _ret += f"batch_size={self.batch_size}, "
        _ret += f"num_batches={self.num_batches})"
        return _ret

    def generate(self, seed: int = None):
        state = np.random.RandomState(seed)

        # NOTE(tawara): Generate randomized positive tuples for each class
        # get_pair_list = lambda lst, sz: [lst[i:i+sz] for i in range(0, len(lst), sz)]
        def get_pair_list(lst, sz):
            return [lst[i:i+sz] for i in range(0, len(lst), sz)]

        def round_down(num, div):
            return num - (num % div)

        pairs = []
        for spk, utts in self.spk2utt_dict_closed.items():
            num_segments = round_down(min(len(utts), self.max_seg_per_spk), self.num_pair)
            if self.shuffle:
                utts = state.permutation(utts)
            pairs.extend(get_pair_list(utts[:num_segments], self.num_pair))
        if self.shuffle:
            pairs = state.permutation(pairs).tolist()

        # NOTE(tawara): Generate batches preventing any pairs of the same class in the same batch.
        #               This is simply implemented by dropping the samples from the same class.
        #               Note that the number of batches might be reduced here.
        key_list = []
        mix_spk = []
        logging.info(len(pairs))
        for pair in pairs:
            startbatch = len(mix_spk) - len(mix_spk) % self.batch_size
            spk = self.utt2spk_dict[pair[0]]
            if spk not in mix_spk[startbatch:] or not self.prevent_same_speaker:
                key_list.append(pair)
                mix_spk.append(spk)
        '''
        for utt in self.utt_list:
            keys = [utt]
            spk = self.utt2spk_dict[utt]

            if self.shuffle:
                spks = state.choice(
                    list(set(self.spk2utt_dict_closed) - {spk}),
                    self.num_pair - 1,
                    replace=False,
                )
            else:
                spks = list(set(self.spk2utt_dict_closed) - {spk})[: self.num_pair - 1]

            for spk2 in spks:
                utts = self.spk2utt_dict_closed[spk2]
                if self.shuffle:
                    utt2 = state.choice(utts, 1)[0]
                else:
                    utt2 = utts[0]
                keys.append(utt2)
            key_list.append(tuple(keys))
        '''
        org_num_batches = self.num_batches
        self.num_batches = len(key_list) // self.batch_size
        if org_num_batches != self.num_batches:
            logging.info(f"N-Batches is modified {org_num_batches} to {self.num_batches} "
                         f"to prevent the same speaker in each batch")
        # NOTE(kamo): Always drop last if not divisible
        batches = [
            tuple(key_list[i * self.batch_size : (i + 1) * self.batch_size])
            for i in range(self.num_batches)
        ]
        if self.distributed:
            world_size = torch.distributed.get_world_size()
            rank = torch.distributed.get_rank()
            batches = [batch[rank::world_size] for batch in batches]
        return batches

    def __len__(self):
        return self.num_batches

    def __iter__(self):
        # Don't use for this task
        raise NotImplementedError
